{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0ef1f98-1ea2-43c8-a107-baffe0a683c2",
   "metadata": {},
   "source": [
    "# Fifth attempt at a world model\n",
    "## Basic Process"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cf9e38bc-4bf6-4d76-9d88-a6bc2b1f5844",
   "metadata": {},
   "source": [
    "Changes since Model IV:\n",
    "- Include cost calculator in predictor.  We aren't more interested in predicting the next state, we're interested in predicting the difference between the next and current cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56c5a29d-5b2c-464c-aaac-51b68ef45eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-09 16:38:29.108038: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-09 16:38:29.176956: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-09 16:38:29.897487: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Settings and setup\n",
    "import tensorflow as tf\n",
    "\n",
    "for gpu in tf.config.list_physical_devices('GPU'):\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "# Main Settings\n",
    "ROLE = \"TAGGER\"\n",
    "PRED_FRAMES_IN_ADVANCE = 10\n",
    "\n",
    "# Encoder Settings\n",
    "IMG_INPUT_SHAPE=(640, 640, 3)\n",
    "\n",
    "NUM_DBS = 8\n",
    "NUM_LAYERS_PER_DB = 1\n",
    "NUM_FILTERS_PER_LAYER_OF_DB = 2\n",
    "\n",
    "DOWNSCALING_LAYERS = 4\n",
    "DOWNSCALING_FACTOR = 2\n",
    "OUTPUT_SIZE = 64\n",
    "ENC_DROPOUT = 0.25\n",
    "\n",
    "# PREDICTOR\n",
    "PRED_POSSIBLE_OUTCOMES = 8\n",
    "PRED_ACTION_LAYERS = 2\n",
    "PRED_ENCODED_LAYERS = 2\n",
    "PRED_INTER_LAYERS = 2\n",
    "PRED_INTER_LAYER_SIZE = 2 * PRED_POSSIBLE_OUTCOMES * OUTPUT_SIZE # This should scale to the number of predictions it makes and the size of each prediction\n",
    "PRED_DROPOUT = 0.25\n",
    "\n",
    "# Cost Calculator Settings\n",
    "CC_INTER_LAYERS = 6\n",
    "CC_INTER_LAYER_SIZE = OUTPUT_SIZE * 4\n",
    "CC_DROPOUT = 0.25\n",
    "CC_TOPLINE_DROPOUT = 0.5 # This dropout should address the difficulties and uncertenties of the predictors job by making this encoder provide somewhat generalizable encodings\n",
    "\n",
    "# Action Generator Settings\n",
    "ACTIONS = 1\n",
    "NUM_POSSIBLE_ACTIONS = 100\n",
    "ACTION_SPACE_SMOOTHING = 5\n",
    "ACT_INTER_LAYERS = 2 + PRED_INTER_LAYERS + CC_INTER_LAYERS # So that it is large enough to understand the other two models in play\n",
    "ACT_INTER_LAYER_SIZE = 1024\n",
    "ACT_DROPOUT = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64458f8c-cece-4be4-bf34-147ca1ed8834",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8779741b-ffda-4856-a096-b653fab49025",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-09 16:38:30.884821: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22161 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:18:00.0, compute capability: 8.9\n",
      "2023-11-09 16:38:30.885363: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22451 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:af:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 640, 640, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 640, 640, 2)          488       ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)           (None, 640, 640, 4)          204       ['conv2d[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 640, 640, 6)          0         ['conv2d[0][0]',              \n",
      "                                                                     'conv2d_1[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)           (None, 640, 640, 4)          28        ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2  (None, 320, 320, 4)          0         ['conv2d_2[0][0]']            \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)           (None, 320, 320, 6)          606       ['max_pooling2d[0][0]']       \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate  (None, 320, 320, 10)         0         ['max_pooling2d[0][0]',       \n",
      " )                                                                   'conv2d_3[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)           (None, 320, 320, 6)          66        ['concatenate_1[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPoolin  (None, 160, 160, 6)          0         ['conv2d_4[0][0]']            \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)           (None, 160, 160, 8)          1208      ['max_pooling2d_1[0][0]']     \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate  (None, 160, 160, 14)         0         ['max_pooling2d_1[0][0]',     \n",
      " )                                                                   'conv2d_5[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)           (None, 160, 160, 8)          120       ['concatenate_2[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPoolin  (None, 80, 80, 8)            0         ['conv2d_6[0][0]']            \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)           (None, 80, 80, 10)           2010      ['max_pooling2d_2[0][0]']     \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate  (None, 80, 80, 18)           0         ['max_pooling2d_2[0][0]',     \n",
      " )                                                                   'conv2d_7[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)           (None, 80, 80, 10)           190       ['concatenate_3[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPoolin  (None, 40, 40, 10)           0         ['conv2d_8[0][0]']            \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)           (None, 40, 40, 12)           3012      ['max_pooling2d_3[0][0]']     \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate  (None, 40, 40, 22)           0         ['max_pooling2d_3[0][0]',     \n",
      " )                                                                   'conv2d_9[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)          (None, 40, 40, 12)           276       ['concatenate_4[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling2d_4 (MaxPoolin  (None, 20, 20, 12)           0         ['conv2d_10[0][0]']           \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)          (None, 20, 20, 14)           4214      ['max_pooling2d_4[0][0]']     \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate  (None, 20, 20, 26)           0         ['max_pooling2d_4[0][0]',     \n",
      " )                                                                   'conv2d_11[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)          (None, 20, 20, 14)           378       ['concatenate_5[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling2d_5 (MaxPoolin  (None, 10, 10, 14)           0         ['conv2d_12[0][0]']           \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)          (None, 10, 10, 16)           5616      ['max_pooling2d_5[0][0]']     \n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate  (None, 10, 10, 30)           0         ['max_pooling2d_5[0][0]',     \n",
      " )                                                                   'conv2d_13[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)          (None, 10, 10, 16)           496       ['concatenate_6[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling2d_6 (MaxPoolin  (None, 5, 5, 16)             0         ['conv2d_14[0][0]']           \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)          (None, 5, 5, 18)             7218      ['max_pooling2d_6[0][0]']     \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate  (None, 5, 5, 34)             0         ['max_pooling2d_6[0][0]',     \n",
      " )                                                                   'conv2d_15[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)          (None, 5, 5, 18)             630       ['concatenate_7[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling2d_7 (MaxPoolin  (None, 2, 2, 18)             0         ['conv2d_16[0][0]']           \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)          (None, 2, 2, 20)             9020      ['max_pooling2d_7[0][0]']     \n",
      "                                                                                                  \n",
      " concatenate_8 (Concatenate  (None, 2, 2, 38)             0         ['max_pooling2d_7[0][0]',     \n",
      " )                                                                   'conv2d_17[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 35780 (139.77 KB)\n",
      "Trainable params: 35780 (139.77 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Image segmenter base\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def create_dense_block(x, num_of_layers, num_filters, kernel_size=5):\n",
    "    output_layers = [x]\n",
    "    for l in range(num_of_layers):\n",
    "        x = layers.Conv2D(num_filters, kernel_size, activation=\"elu\", padding=\"same\")(x)\n",
    "        output_layers.append(x)\n",
    "        x = layers.Concatenate()(output_layers)\n",
    "    return x\n",
    "\n",
    "def create_transistion_down(x, layer_size_change, num_filters=None):\n",
    "    if num_filters:\n",
    "        x = layers.Conv2D(num_filters, 1, activation=\"elu\", padding=\"same\")(x)\n",
    "    \n",
    "    return layers.MaxPool2D(layer_size_change)(x)\n",
    "\n",
    "# Keep track for skip connections and other things\n",
    "skip_connections = []\n",
    "previous_filters = [NUM_LAYERS_PER_DB * NUM_FILTERS_PER_LAYER_OF_DB]\n",
    "\n",
    "# Num of filters: 4 (layers) * 32 (idk) + 64 (previous filters)\n",
    "# Should prob figure out why this is\n",
    "\n",
    "model_in = layers.Input(shape=IMG_INPUT_SHAPE)\n",
    "x = layers.Conv2D(previous_filters[-1], 9, activation=\"elu\", padding=\"same\")(model_in)\n",
    "\n",
    "for i in range(NUM_DBS):\n",
    "    previous_filters.append(NUM_LAYERS_PER_DB * NUM_FILTERS_PER_LAYER_OF_DB + previous_filters[-1])\n",
    "    x = create_dense_block(x, NUM_LAYERS_PER_DB, previous_filters[-1])\n",
    "    skip_connections.append(x)\n",
    "    x = create_transistion_down(x, 2, num_filters=(previous_filters[-1]))\n",
    "\n",
    "\n",
    "previous_filters.append(NUM_LAYERS_PER_DB * NUM_FILTERS_PER_LAYER_OF_DB + previous_filters[-1])\n",
    "x = create_dense_block(x, NUM_LAYERS_PER_DB, previous_filters[-1]) \n",
    "\n",
    "dense_net = models.Model(model_in, x)\n",
    "\n",
    "dense_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4136aa64-ad4b-4d0a-8a16-9218289b1c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " random_flip (RandomFlip)    (None, 640, 640, 3)       0         \n",
      "                                                                 \n",
      " model (Functional)          (None, 2, 2, 38)          35780     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 152)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              156672    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 889732 (3.39 MB)\n",
      "Trainable params: 889732 (3.39 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Encoder (Simple downscaler)\n",
    "\n",
    "# Added encoder to flatten dense_net\n",
    "encoder = models.Sequential()\n",
    "encoder.add(layers.Input(shape=IMG_INPUT_SHAPE))\n",
    "encoder.add(layers.RandomFlip())\n",
    "encoder.add(dense_net)\n",
    "encoder.add(layers.Flatten())\n",
    "\n",
    "for i in range(DOWNSCALING_LAYERS + 1):\n",
    "    encoder.add(layers.Dense(DOWNSCALING_FACTOR**(DOWNSCALING_LAYERS-i) * OUTPUT_SIZE, activation=\"elu\",)) # kernel_regularizer='l2'))\n",
    "    encoder.add(layers.Dropout(ENC_DROPOUT))\n",
    "\n",
    "# encoder.add(layers.BatchNormalization(axis=-1))\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd4872c-0152-4ca3-a285-70083d5cb72f",
   "metadata": {},
   "source": [
    "# Cost Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "361973ee-4a46-421d-a532-a21313fd8860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dropout_5 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               16640     \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 386817 (1.48 MB)\n",
      "Trainable params: 386817 (1.48 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cost_calculator = models.Sequential()\n",
    "cost_calculator.add(layers.Input(shape=OUTPUT_SIZE))\n",
    "cost_calculator.add(layers.Dropout(CC_TOPLINE_DROPOUT))\n",
    "\n",
    "# Add internal layers\n",
    "for _ in range(CC_INTER_LAYERS):\n",
    "    cost_calculator.add(layers.Dense(CC_INTER_LAYER_SIZE, activation=\"elu\",)) # kernel_regularizer='l2'))\n",
    "    cost_calculator.add(layers.Dropout(CC_DROPOUT))\n",
    "\n",
    "cost_calculator.add(layers.Dense(CC_INTER_LAYER_SIZE//2)) #, activation=\"elu\"))\n",
    "cost_calculator.add(layers.Dense(CC_INTER_LAYER_SIZE//4)) #, activation=\"elu\"))\n",
    "cost_calculator.add(layers.Dense(1, activation=\"sigmoid\")) #, activation=\"elu\"))\n",
    "\n",
    "cost_calculator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d720b06d-d2dd-43e6-8cf7-3b12025a5912",
   "metadata": {},
   "source": [
    "# Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e81c043-2a8c-4297-b625-eaa1a444cf03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)        [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " input_5 (InputLayer)        [(None, 64)]                 0         []                            \n",
      "                                                                                                  \n",
      " dense_14 (Dense)            (None, 512)                  1024      ['input_4[0][0]']             \n",
      "                                                                                                  \n",
      " dense_16 (Dense)            (None, 512)                  33280     ['input_5[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)        (None, 512)                  0         ['dense_14[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)        (None, 512)                  0         ['dense_16[0][0]']            \n",
      "                                                                                                  \n",
      " dense_15 (Dense)            (None, 512)                  262656    ['dropout_12[0][0]']          \n",
      "                                                                                                  \n",
      " dense_17 (Dense)            (None, 512)                  262656    ['dropout_14[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)        (None, 512)                  0         ['dense_15[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)        (None, 512)                  0         ['dense_17[0][0]']            \n",
      "                                                                                                  \n",
      " tf.concat (TFOpLambda)      (None, 1024)                 0         ['dropout_13[0][0]',          \n",
      "                                                                     'dropout_15[0][0]']          \n",
      "                                                                                                  \n",
      " dense_18 (Dense)            (None, 1024)                 1049600   ['tf.concat[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)        (None, 1024)                 0         ['dense_18[0][0]']            \n",
      "                                                                                                  \n",
      " dense_19 (Dense)            (None, 1024)                 1049600   ['dropout_16[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)        (None, 1024)                 0         ['dense_19[0][0]']            \n",
      "                                                                                                  \n",
      " dense_20 (Dense)            (None, 512)                  524800    ['dropout_17[0][0]']          \n",
      "                                                                                                  \n",
      " dense_21 (Dense)            (None, 256)                  131328    ['dense_20[0][0]']            \n",
      "                                                                                                  \n",
      " dense_22 (Dense)            (None, 128)                  32896     ['dense_21[0][0]']            \n",
      "                                                                                                  \n",
      " tf.compat.v1.shape (TFOpLa  (2,)                         0         ['input_4[0][0]']             \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " dense_23 (Dense)            (None, 8)                    1032      ['dense_22[0][0]']            \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (  ()                           0         ['tf.compat.v1.shape[0][0]']  \n",
      " SlicingOpLambda)                                                                                 \n",
      "                                                                                                  \n",
      " tf.reshape (TFOpLambda)     (None, 8, 1)                 0         ['dense_23[0][0]',            \n",
      "                                                                     'tf.__operators__.getitem[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3348872 (12.77 MB)\n",
      "Trainable params: 3348872 (12.77 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build new predictor model\n",
    "\n",
    "# Handle actionspace\n",
    "keys = layers.Input(shape=ACTIONS)\n",
    "pred_keys = keys\n",
    "\n",
    "for _ in range(PRED_ACTION_LAYERS):\n",
    "    pred_keys = layers.Dense(PRED_INTER_LAYER_SIZE//2, activation=\"elu\",)(pred_keys)\n",
    "    pred_keys = layers.Dropout(PRED_DROPOUT)(pred_keys)\n",
    "\n",
    "# Handle encoded frame\n",
    "encoded_frame = layers.Input(shape=OUTPUT_SIZE)\n",
    "pred_frame = encoded_frame\n",
    "\n",
    "for _ in range(PRED_ENCODED_LAYERS):\n",
    "    pred_frame = layers.Dense(PRED_INTER_LAYER_SIZE//2, activation=\"elu\",)(pred_frame)\n",
    "    pred_frame = layers.Dropout(PRED_DROPOUT)(pred_frame)\n",
    "\n",
    "# Combine them\n",
    "predictor = tf.concat([pred_keys, pred_frame], axis=-1)\n",
    "for _ in range(PRED_INTER_LAYERS):\n",
    "    predictor = layers.Dense(PRED_INTER_LAYER_SIZE, activation=\"elu\",)(predictor)\n",
    "    predictor = layers.Dropout(PRED_DROPOUT)(predictor)\n",
    "\n",
    "predictor = layers.Dense(PRED_POSSIBLE_OUTCOMES * OUTPUT_SIZE, activation=\"elu\")(predictor)\n",
    "predictor = layers.Dense(PRED_POSSIBLE_OUTCOMES * (OUTPUT_SIZE//2), activation=\"elu\")(predictor)\n",
    "predictor = layers.Dense(PRED_POSSIBLE_OUTCOMES * (OUTPUT_SIZE//4), activation=\"elu\")(predictor)\n",
    "predictor = layers.Dense(PRED_POSSIBLE_OUTCOMES, activation=\"sigmoid\")(predictor)\n",
    "\n",
    "predictor = tf.reshape(predictor, (tf.shape(keys)[0], PRED_POSSIBLE_OUTCOMES, 1))\n",
    "\n",
    "predictor = models.Model([keys, encoded_frame], predictor)\n",
    "predictor.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fed330-69b5-4ed6-b1e7-227fa4954474",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4151f1c3-3852-4453-bc4b-88d083a750b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "ENCODER_AND_COST_AND_PRED_LR = 1 * 10**-3\n",
    "COST_LR = 1 * 10**-4\n",
    "PRED_LR = 1 * 10**-4\n",
    "\n",
    "MAX_EPOCHS = 200\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11328cac-e8a3-4292-b04b-7a6ede8a5427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import numpy as np\n",
    "\n",
    "with tf.device(\"cpu:0\"):\n",
    "    runs = pd.read_csv(\"game_runs.csv\")[[\"Run\", \"Winner\"]].sample(frac=1).values.tolist()\n",
    "    all_frames = []\n",
    "    all_next_frames = []\n",
    "    all_input = []\n",
    "    all_scores = []\n",
    "    for run, winner in runs[:75]:\n",
    "        frames = tf.cast(tf.io.parse_tensor(tf.io.read_file(\"runs/%d/frames.proto_tensor\" % run), tf.uint8), tf.float16)/255.0\n",
    "        all_frames.append(frames)\n",
    "    \n",
    "        if ROLE == \"TAGGER\":\n",
    "            all_input.append(tf.io.parse_tensor(tf.io.read_file(\"runs/%d/tagger_inputs.proto_tensor\" % run), float))\n",
    "        else:\n",
    "            all_input.append(tf.io.parse_tensor(tf.io.read_file(\"runs/%d/taggee_inputs.proto_tensor\" % run), float))\n",
    "    \n",
    "        tagger_poses = tf.io.parse_tensor(tf.io.read_file(\"runs/%d/tagger_poses.proto_tensor\" % run), float)\n",
    "        taggee_poses = tf.io.parse_tensor(tf.io.read_file(\"runs/%d/taggee_poses.proto_tensor\" % run), float)\n",
    "    \n",
    "        scores_normalizer = tf.math.sqrt(tf.cast(tf.math.square(IMG_INPUT_SHAPE[0]) + tf.math.square(IMG_INPUT_SHAPE[1]), float))\n",
    "        # scores_normalizer = IMG_INPUT_SHAPE[0]\n",
    "        \n",
    "        if ROLE == \"TAGGER\":\n",
    "            scores = tf.math.sqrt(tf.math.reduce_sum(tf.math.square(tagger_poses - taggee_poses), axis=1))/scores_normalizer\n",
    "        else:\n",
    "            scores = (scores_normalizer - tf.math.sqrt(tf.math.reduce_sum(tf.math.square(tagger_poses - taggee_poses), axis=1)))/scores_normalizer\n",
    "        \n",
    "        # if winner != ROLE:\n",
    "        #     # # Set all last scores to be 1\n",
    "        #     # # scores = tf.concat([scores[:-5:], tf.repeat(tf.constant([1.0]), repeats=5)], axis=0)\n",
    "            \n",
    "        #     # Make scores at the end more linear\n",
    "        #     dist = 0.75 - scores[-10].numpy()\n",
    "        #     smoothed_end = [scores[-10].numpy() + (dist / 10 * (i + 1)) for i in range(10)]\n",
    "        #     scores = tf.concat([scores[:-10:], tf.constant(smoothed_end, float)], axis=0)\n",
    "\n",
    "        all_scores.append(scores)\n",
    "        print(run)\n",
    "\n",
    "    # all_next_frames = tf.concat([frame[PRED_FRAMES_IN_ADVANCE:] for frame in all_frames], axis=0)\n",
    "    all_frames = tf.concat([frame[:-PRED_FRAMES_IN_ADVANCE] for frame in all_frames], axis=0)\n",
    "    all_input = tf.cast(tf.concat([input[:-PRED_FRAMES_IN_ADVANCE] for input in all_input], axis=0), tf.float16)\n",
    "    all_future_score_diffs = tf.cast(tf.concat([score[PRED_FRAMES_IN_ADVANCE:] - score[:-PRED_FRAMES_IN_ADVANCE] for score in all_scores], axis=0), tf.float16)\n",
    "    all_scores = tf.cast(tf.concat([score[:-PRED_FRAMES_IN_ADVANCE] for score in all_scores], axis=0), tf.float16)\n",
    "\n",
    "    # Normalize scores\n",
    "    all_scores = tf.expand_dims(all_scores, axis=-1)\n",
    "    all_scores = tf.numpy_function(lambda x: RobustScaler().fit_transform(x), [all_scores], float)\n",
    "    all_scores_min, all_scores_max = tf.math.reduce_min(all_scores, keepdims=True), tf.math.reduce_max(all_scores, keepdims=True)\n",
    "    all_scores = (all_scores - all_scores_min) / (all_scores_max - all_scores_min)\n",
    "    all_scores = tf.cast(all_scores, tf.float16)\n",
    "\n",
    "    # Normalize future scores\n",
    "    all_future_score_diffs = tf.expand_dims(all_future_score_diffs, axis=-1)\n",
    "    all_future_score_diffs = tf.numpy_function(lambda x: RobustScaler().fit_transform(x), [all_future_score_diffs], float)\n",
    "    all_future_score_diffs_min, all_future_score_diffs_max = tf.math.reduce_min(all_future_score_diffs, keepdims=True), tf.math.reduce_max(all_future_score_diffs, keepdims=True)\n",
    "    all_future_score_diffs = (all_future_score_diffs - all_future_score_diffs_min) / (all_future_score_diffs_max - all_future_score_diffs_min)\n",
    "    all_future_score_diffs = tf.cast(all_future_score_diffs, tf.float16)\n",
    "    \n",
    "    # Sum the inputs\n",
    "    mult_inputs = tf.constant([[1, -1]], tf.float16)\n",
    "    print(all_input.shape, mult_inputs.shape)\n",
    "    all_input *= mult_inputs\n",
    "    all_input = tf.math.reduce_sum(all_input, axis=-1, keepdims=True)\n",
    "\n",
    "    # Round the inputs\n",
    "    all_input *= 100\n",
    "    all_input = tf.math.round(all_input)/100\n",
    "    \n",
    "    print(tf.shape(all_frames), tf.shape(all_input), tf.shape(all_scores), tf.shape(all_future_score_diffs)) #, tf.shape(all_delta_scores))\n",
    "    print(tf.math.reduce_max(all_scores), tf.math.reduce_min(all_scores)) # , tf.math.reduce_max(all_delta_scores), tf.math.reduce_min(all_delta_scores))\n",
    "\n",
    "    # Plot histogram of scores\n",
    "    plt.hist(all_scores[:, 0].numpy(), bins=100)\n",
    "    plt.title(\"Score Distribution\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot histogram of input\n",
    "    plt.hist(all_input[:, 0].numpy(), bins=100)\n",
    "    plt.title(\"Input Distribution\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot histogram of score changes\n",
    "    plt.hist(all_future_score_diffs[:, 0].numpy(), bins=100)\n",
    "    plt.title(\"Change in Score Distribution\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot how inputs relate to scores\n",
    "    plt.scatter(all_input[:, 0].numpy(), all_future_score_diffs[:, 0].numpy())\n",
    "    a, b = np.polyfit(all_input[:, 0].numpy().astype(float), all_future_score_diffs[:, 0].numpy().astype(float), 1)\n",
    "    plt.plot(all_input[:, 0].numpy(), a*all_input[:, 0].numpy()+b, label=\"LofBF: m: %.5f, b: %.1f\" % (a, b), c=\"tab:orange\")\n",
    "    plt.xlabel(\"Current Input\")\n",
    "    plt.ylabel(\"Change in Score\")\n",
    "    plt.title(\"Inputs Vs Changes in Future Scores\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # plt.hist(all_delta_scores[:, 0].numpy(), bins=100)\n",
    "    # plt.title(\"Delta Score Distribution\")\n",
    "    # plt.show()\n",
    "\n",
    "    # Make DS\n",
    "    big_train_ds = tf.data.Dataset.from_tensor_slices((all_frames, all_input, all_scores, all_future_score_diffs)) # , all_delta_scores))\n",
    "    big_train_ds = big_train_ds.shuffle(big_train_ds.cardinality())\n",
    "    big_valid_ds = big_train_ds.shard(10, 8)\n",
    "    big_test_ds = big_train_ds.shard(10, 9)\n",
    "    big_train_ds = big_train_ds.take((8 * big_train_ds.cardinality())//10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dc5ec5-cb2b-4f33-b102-dd0c2d07a9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the histogram of inputs completely even\n",
    "\n",
    "# Approach: using a really small epison, ensure that there is only one x value within the range (-1, -1 + e), (-1 + e, -1 + 2*e), ...  The epsilon should be really really small "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed071f9-6abf-4e63-94e3-d317ff3a5e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train encoder, predictor, and cost calculaor together\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "def minimum_mse(y_true, y_pred):\n",
    "    se = tf.math.square(tf.expand_dims(y_true, axis=1) - y_pred)\n",
    "    mse = tf.math.reduce_mean(se, axis=2) # because axis 0 is batch and 1 is the 4 predictions\n",
    "    return tf.math.reduce_min(mse, axis=1) # because axis 0 is batch\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    se = tf.math.square(tf.expand_dims(y_true, axis=1) - y_pred)\n",
    "    return tf.math.reduce_mean(se, axis=[1, 2]) # Because axis 0 is batch axis\n",
    "\n",
    "def combined_loss(y_true, y_pred, mse_weight=0.25):\n",
    "    return minimum_mse(y_true, y_pred) + mse_weight * mse(y_true, y_pred)\n",
    "\n",
    "# Comment out these two lines to train the encoder from scratch\n",
    "# encoder = tf.keras.models.load_model(\"encoder_%s.keras\" % ROLE)\n",
    "# cost_calculator = tf.keras.models.load_model(\"cost_calculator_%s.keras\" % ROLE)\n",
    "# predictor = tf.keras.models.load_model(\"predictor_%s.keras\" % ROLE)\n",
    "\n",
    "frame_and_score = lambda w, x, y, z: (w, y)\n",
    "enc_cost_calc = models.Sequential([layers.Input(shape=IMG_INPUT_SHAPE), encoder, cost_calculator])\n",
    "enc_cost_calc.compile(optimizers.Adam(learning_rate=ENCODER_AND_COST_AND_PRED_LR), loss=\"mse\", metrics=[\"mse\"])\n",
    "enc_cost_calc.summary()\n",
    "\n",
    "train_ds_frame_and_score = big_train_ds.map(frame_and_score, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False).batch(BATCH_SIZE).cache()\n",
    "valid_ds_frame_and_score = big_valid_ds.map(frame_and_score, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False).batch(BATCH_SIZE).cache()\n",
    "\n",
    "frame_key_next_frame = lambda w, x, y, z: ((w, x), z)\n",
    "cur_frame_in = layers.Input(shape=IMG_INPUT_SHAPE)\n",
    "cur_key_in = layers.Input(shape=ACTIONS)\n",
    "enc_pred_enc = encoder(cur_frame_in)\n",
    "enc_pred_enc = predictor((cur_key_in, enc_pred_enc))\n",
    "enc_pred_enc = models.Model([cur_frame_in, cur_key_in], enc_pred_enc)\n",
    "enc_pred_enc.compile(optimizers.Adam(learning_rate=ENCODER_AND_COST_AND_PRED_LR), loss=combined_loss, metrics=[minimum_mse, mse])\n",
    "enc_pred_enc.summary()\n",
    "\n",
    "train_ds_frame_key_next_frame = big_train_ds.map(frame_key_next_frame, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False).batch(BATCH_SIZE).cache()\n",
    "valid_ds_frame_key_next_frame = big_valid_ds.map(frame_key_next_frame, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False).batch(BATCH_SIZE).cache()\n",
    "\n",
    "best_encoder, best_cost_calculator, best_pred = None, None, None\n",
    "min_cc_losses, min_pred_losses = 10**10, 10**10\n",
    "\n",
    "time_without_save = 0\n",
    "\n",
    "for i in range(MAX_EPOCHS):\n",
    "    train_iter_frame_and_score = iter(train_ds_frame_and_score)\n",
    "    train_iter_frame_key_next_frame = iter(train_ds_frame_key_next_frame)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # Fit cost calculator and encoder to one batch\n",
    "            data = train_iter_frame_and_score.next()\n",
    "            enc_cc_loss = enc_cost_calc.train_on_batch(\n",
    "                data[0], data[1]\n",
    "            )\n",
    "\n",
    "            # Fit predictor and encoder to one batch\n",
    "            data = train_iter_frame_key_next_frame.next()\n",
    "            enc_pred_enc.train_on_batch(\n",
    "                data[0], data[1]\n",
    "            )\n",
    "        except StopIteration:\n",
    "            break\n",
    "\n",
    "    enc_cc_loss = enc_cost_calc.evaluate(valid_ds_frame_and_score, return_dict=True)[\"loss\"]\n",
    "    enc_pred_loss = enc_pred_enc.evaluate(valid_ds_frame_key_next_frame, return_dict=True)[\"loss\"]\n",
    "            \n",
    "    if enc_cc_loss <= min_cc_losses and enc_pred_loss <= min_pred_losses:\n",
    "        print(i, \"Saving\")\n",
    "\n",
    "        min_cc_losses = enc_cc_loss\n",
    "        min_pred_losses = enc_pred_loss\n",
    "        \n",
    "        min_cc_losses\n",
    "        encoder.save(\"encoder_%s.keras\" % ROLE)\n",
    "        cost_calculator.save(\"cost_calculator_%s.keras\" % ROLE)\n",
    "        predictor.save(\"predictor_%s.keras\" % ROLE)\n",
    "        time_without_save = 0\n",
    "    else:\n",
    "        print(i)\n",
    "        time_without_save += 1\n",
    "\n",
    "    if time_without_save > MAX_EPOCHS//10:\n",
    "        break\n",
    "\n",
    "encoder = tf.keras.models.load_model(\"encoder_%s.keras\" % ROLE)\n",
    "cost_calculator = tf.keras.models.load_model(\"cost_calculator_%s.keras\" % ROLE)\n",
    "predictor = tf.keras.models.load_model(\"predictor_%s.keras\" % ROLE, custom_objects={\"combined_loss\": combined_loss, \"minimum_mse\": minimum_mse, \"mse\": mse})\n",
    "\n",
    "encoder.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcf733b-6ccd-4b37-a806-2d9ea87014b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train cost calculator without encoder and lower lr\n",
    "\n",
    "cost_calculator.compile(optimizers.Adam(learning_rate=COST_LR), loss=\"mse\", metrics=[\"mse\"])\n",
    "cost_calculator.summary()\n",
    "\n",
    "encodings_and_score = lambda w, x, y, z: (encoder(w), y)\n",
    "\n",
    "encoding_cost_train_ds = big_train_ds.batch(BATCH_SIZE).map(encodings_and_score, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False).cache()\n",
    "encoding_cost_valid_ds = big_valid_ds.batch(BATCH_SIZE).map(encodings_and_score, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False).cache()\n",
    "\n",
    "cost_calculator.fit(\n",
    "    encoding_cost_train_ds,\n",
    "    validation_data=encoding_cost_valid_ds,\n",
    "    epochs=MAX_EPOCHS,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=MAX_EPOCHS//10, restore_best_weights=True)]\n",
    ")\n",
    "\n",
    "cost_calculator.save(\"cost_calculator_%s.keras\" % ROLE)\n",
    "cost_calculator.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831d83dd-48e4-402d-9638-f4e40cd6245e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train prediction\n",
    "\n",
    "with tf.device(\"cpu:0\"):\n",
    "    apply_encoder = lambda w, x, y, z: ((y, tf.cast(encoder(w), tf.float16)), z)\n",
    "    pred_train_ds = big_train_ds.batch(BATCH_SIZE).map(apply_encoder, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False).cache()\n",
    "    pred_valid_ds = big_valid_ds.batch(BATCH_SIZE).map(apply_encoder, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False).cache()\n",
    "\n",
    "with tf.device(\"gpu:0\"):\n",
    "    predictor.compile(optimizers.Adam(learning_rate=PRED_LR), loss=combined_loss, metrics=[minimum_mse, mse])\n",
    "    predictor.summary()\n",
    "    \n",
    "    predictor.fit(\n",
    "        pred_train_ds,\n",
    "        validation_data=pred_valid_ds,\n",
    "        epochs=MAX_EPOCHS,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(patience=MAX_EPOCHS//10, restore_best_weights=True)]\n",
    "    )\n",
    "\n",
    "predictor.save(\"predictor_%s.keras\" % ROLE)\n",
    "predictor.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e09e02-3764-45bf-ba73-60ccd4648d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train action generator\n",
    "\n",
    "@tf.function\n",
    "def define_action(encoded_frames, smoothing=ACTION_SPACE_SMOOTHING, num_actions=NUM_POSSIBLE_ACTIONS):\n",
    "    actions = tf.cast(tf.expand_dims(tf.linspace(-1, 1, num_actions), axis=-1), tf.float16)\n",
    "    print(tf.shape(actions))\n",
    "    actions = tf.repeat(tf.expand_dims(actions, axis=0), repeats=tf.shape(encoded_frames)[0], axis=0)\n",
    "    print(tf.shape(actions))\n",
    "\n",
    "    # Now actions has shape (batch_size, num_actions, 1)\n",
    "    # Make it have shape (num_actions, batch_size, 1)\n",
    "    actions = tf.transpose(actions, perm=[1, 0, 2])\n",
    "\n",
    "    predictions = tf.map_fn(\n",
    "        lambda x: predictor((x, encoded_frames)),\n",
    "        actions,\n",
    "        fn_output_signature=tf.TensorSpec((None, PRED_POSSIBLE_OUTCOMES, 1), float)\n",
    "    )\n",
    "\n",
    "    # Above returns the predicted scores in shape (num_actions, batch_size, num_predictons, 1)\n",
    "    # Reshape to (num_actions, batch_size, num_predictons) then (batch_size, num_actions, num_predictons) then avg to (batch_size, num_actions)\n",
    "    predictions = predictions[:, :, :, 0]\n",
    "    predictions = tf.transpose(predictions, perm=[1, 0, 2])\n",
    "    avg_predictions = tf.math.reduce_mean(predictions, axis=-1)\n",
    "\n",
    "    # Use numpy to smooth each set of actions\n",
    "    act_num_actions = num_actions - 2 * smoothing\n",
    "    smoothing_func = lambda y: tf.numpy_function(lambda x: (np.convolve(x, np.ones(smoothing), 'same')[smoothing:-smoothing]/smoothing).astype(np.single), [y], float)\n",
    "    smoothed_predictions = tf.map_fn(smoothing_func, avg_predictions, fn_output_signature=tf.TensorSpec((act_num_actions,), float))\n",
    "    \n",
    "    # return the argmin (normalized to actual input) of the smoothed function.  In addition, return the cost tensors\n",
    "    pos_actions = tf.cast((tf.range(act_num_actions) - (act_num_actions//2))/(num_actions//2), float)\n",
    "    moves = tf.math.argmin(smoothed_predictions, axis=-1)\n",
    "    print(moves, pos_actions)\n",
    "    moves = tf.map_fn(lambda x: tf.expand_dims(pos_actions[x], axis=-1), moves, fn_output_signature=tf.TensorSpec((1,), float))\n",
    "    return moves, predictions, smoothed_predictions\n",
    "\n",
    "\n",
    "class KerasGenActionLayer(layers.Layer):\n",
    "    def call(self, x, training=None):\n",
    "        print(tf.shape(x))\n",
    "        return define_action(x, smoothing=ACTION_SPACE_SMOOTHING, num_actions=NUM_POSSIBLE_ACTIONS)[0]\n",
    "\n",
    "\n",
    "inp = layers.Input(shape=IMG_INPUT_SHAPE)\n",
    "action_gen = encoder(inp)\n",
    "action_gen = KerasGenActionLayer()(action_gen)\n",
    "\n",
    "action_gen = models.Model(inp, action_gen)\n",
    "\n",
    "action_gen.summary()\n",
    "action_gen.save(\"action_gen_%s.keras\" % ROLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b9c078-a57b-400c-acb7-1bce4720485e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sanity test: how does turning effect score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "test_cc_ds = big_valid_ds.batch(16).take(1)\n",
    "\n",
    "for elements in test_cc_ds.map(lambda w, x, y, z: (encoder(w), w, y, z)):\n",
    "    move, costs, smoothed_costs = define_action(elements[0])\n",
    "    for i in range(tf.shape(move)[0]):\n",
    "        fig, ax = plt.subplots(1, 3, figsize=(15,10))\n",
    "\n",
    "        # Plot image\n",
    "        ax[0].imshow(elements[1][i].numpy().astype(float))\n",
    "        ax[0].set_title(\"Current frame\")\n",
    "\n",
    "        # Plot scores for each action\n",
    "        ax[1].plot(tf.range(NUM_POSSIBLE_ACTIONS)/(NUM_POSSIBLE_ACTIONS//2) - 1, costs[i])\n",
    "        ax[1].set_title(\"Predicted future costs\")\n",
    "        ax[1].set_xlabel(\"Steering input\")\n",
    "        ax[1].set_ylabel(\"Predicted Change in Utility (Lower is Better)\")\n",
    "\n",
    "        # Plot smoothed scores for each action\n",
    "        num_actions = NUM_POSSIBLE_ACTIONS - 2 * ACTION_SPACE_SMOOTHING\n",
    "        ax[2].plot((tf.range(num_actions) - (num_actions//2))/(NUM_POSSIBLE_ACTIONS//2), smoothed_costs[i])\n",
    "        ax[2].set_title(\"Predicted future costs\")\n",
    "        ax[2].set_xlabel(\"Steering input\")\n",
    "        ax[2].set_ylabel(\"Predicted Change in Utility (Lower is Better)\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(move[i])\n",
    "    # print(, tf.shape(costs), tf.shape(smoothed_costs))\n",
    "\n",
    "# test_cc_ds = big_valid_ds.take(10)\n",
    "# for element in test_cc_ds:\n",
    "#     fig, ax = plt.subplots(1, 3, figsize=(15,10))\n",
    "#     ax[0].imshow(element[0].numpy().astype(float))\n",
    "#     ax[0].set_title(\"Current frame\")\n",
    "\n",
    "#     actions = tf.cast(tf.expand_dims((tf.range(200)/100) - 1, axis=-1), tf.float16)\n",
    "#     act_results = tf.map_fn(lambda x: cost_calculator(predictor((x, encoder(tf.expand_dims(element[0], axis=0))))[0, :, :]), actions, fn_output_signature=float)\n",
    "#     actions = tf.cast(actions, float)\n",
    "    \n",
    "#     ax[1].plot(actions[:, 0], act_results[:, :, 0])\n",
    "#     ax[1].set_title(\"Predicted future costs\")\n",
    "#     ax[1].set_xlabel(\"Steering input\")\n",
    "#     ax[1].set_ylabel(\"Predicted Utility (Lower is Better)\")\n",
    "    \n",
    "#     real_cost = tf.cast(element[3][0], float)\n",
    "#     calc_cost = cost_calculator(encoder(tf.expand_dims(element[0], axis=0)))[0, 0]\n",
    "\n",
    "#     # decision_functions = tf.numpy_function(lambda x, y: np.polynomial.polynomial.polyfit(x, np.convolve(y, np.ones(5), 'same')/5, 1), [actions[:, 0], act_results[:, -1, 0]], float)\n",
    "#     # evaled_functions = tf.map_fn(lambda x: tf.math.polyval(list(decision_functions.numpy()), x), actions[:, 0])\n",
    "#     # evaled_functions = tf.transpose(tf.cast(evaled_functions, float))\n",
    "    \n",
    "#     ax[2].plot(actions[5:-5, 0], np.convolve(tf.math.reduce_mean(act_results[:, :, 0], axis=-1).numpy(), np.ones(5), 'same')[10:-10]/10 - calc_cost)\n",
    "#     ax[2].set_title(\"Utility Minimization Function\")\n",
    "#     ax[2].set_xlabel(\"Steering input\")\n",
    "#     ax[2].set_ylabel(\"Predicted Utility (Lower is Better)\")\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "    \n",
    "#     print(\"Real cost, calculated cost, difference\", real_cost.numpy(), calc_cost.numpy(), (real_cost - calc_cost).numpy())\n",
    "\n",
    "#     calc_next_cost = cost_calculator(encoder(tf.expand_dims(element[1], axis=0)))[:, 0]\n",
    "#     pred_next_cost = cost_calculator(predictor((element[2], encoder(tf.expand_dims(element[0], axis=0))))[0, :, :])[:, 0]\n",
    "\n",
    "#     print(\"Caclulated next frame cost, predicted next frame cost cost, difference\", calc_next_cost[0].numpy(), pred_next_cost.numpy(), (calc_next_cost - pred_next_cost).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272606a9-6e44-4ca0-82e1-a683ed7258ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in test_cc_ds.map(lambda w, x, y, z: w):\n",
    "    moves = action_gen(element)[:, 0]\n",
    "print(moves)\n",
    "\n",
    "# Plot histogram of inputs\n",
    "plt.hist(moves, bins=100)\n",
    "plt.title(\"Change in Score Distribution\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bafceb5-e880-4737-8af0-984e20d5eaec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
